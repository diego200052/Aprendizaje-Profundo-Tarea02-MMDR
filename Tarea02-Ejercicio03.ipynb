{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/bereml/riiaa-20-mtl/blob/master/notebooks/1a_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"# Reconocimiento de acciones humanas (4 pts.)\n\nEn este ejercicio debes comparar arquitecturas RNN y CNN para reconocimiento de acciones humanas en el conjunto UCF11. La solución debe cumplir con los siguientes puntos.\n\n* Usar las características convolucionales vistas en clase.\n* Implementar una arquitectura RNN bidireccional con una capa GRU.\n* Implementar una arquitectura CNN con una capa Conv1d.\n* Modificar el tamaño de las capas para que ambos modelos tengan un número similar de parámetros.\n* Discutir el comportamiento durante el entrenamiento y resultados finales en ambos conjuntos.\n","metadata":{}},{"cell_type":"markdown","source":"# Reconocimiento de acciones humanas usando RNNs \n\nCurso: [Aprendizaje Profundo](http://turing.iimas.unam.mx/~gibranfp/cursos/aprendizaje_profundo/). Profesor: [Gibran Fuentes Pineda](http://turing.iimas.unam.mx/~gibranfp/). Ayudantes: [Bere](https://turing.iimas.unam.mx/~bereml/) y [Ricardo](https://turing.iimas.unam.mx/~ricardoml/).\n\n\n---\n---\n\nEn esta libreta entrenaremos un modelo basado en RNNs para reconocimiento de acciones humanas (HAR) en el conjunto [UCF11](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php).\n\n<img src=\"https://www.crcv.ucf.edu/data/youtube_snaps.jpg\" width=800/>\n\nEste ejemplo está basado en las ideas presentadas en [*Long-term Recurrent Convolutional Networks for Visual Recognition and Description*](https://arxiv.org/abs/1411.4389) de 2016 por Donahue et al. ","metadata":{}},{"cell_type":"markdown","source":"## 1 Preparación","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Bibliotecas","metadata":{}},{"cell_type":"code","source":"# Colab\n# https://github.com/TylerYep/torchinfo\n!pip install torchinfo\n# https://zarr.readthedocs.io/en/stable/\n!pip install zarr","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:58:28.873494Z","iopub.execute_input":"2022-11-14T02:58:28.873922Z","iopub.status.idle":"2022-11-14T02:58:52.084801Z","shell.execute_reply.started":"2022-11-14T02:58:28.873839Z","shell.execute_reply":"2022-11-14T02:58:52.083781Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchinfo\n  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting zarr\n  Downloading zarr-2.12.0-py3-none-any.whl (185 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 kB\u001b[0m \u001b[31m854.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting numcodecs>=0.6.4\n  Downloading numcodecs-0.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting asciitree\n  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.7/site-packages (from zarr) (1.21.6)\nRequirement already satisfied: fasteners in /opt/conda/lib/python3.7/site-packages (from zarr) (0.17.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from numcodecs>=0.6.4->zarr) (0.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from numcodecs>=0.6.4->zarr) (4.4.0)\nBuilding wheels for collected packages: asciitree\n  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5050 sha256=d185d0e0058141474c2bfb8e2355558a51273726df19491b67291d9999e2d8ef\n  Stored in directory: /root/.cache/pip/wheels/12/1c/38/0def51e15add93bff3f4bf9c248b94db0839b980b8535e72a0\nSuccessfully built asciitree\nInstalling collected packages: asciitree, numcodecs, zarr\nSuccessfully installed asciitree-0.3.3 numcodecs-0.10.2 zarr-2.12.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# sistema de archivos\nimport os\n# funciones aleatorias\nimport random\n# descomprimir\nimport tarfile\n# sistema de archivos\nfrom os.path import join\n\n# arreglos multidimensionales\nimport numpy as np\n# redes neuronales\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets.utils as tvu\n# almacenamiento de arreglos multidimensionales\nimport zarr\n#redes\nfrom torch.utils.data import DataLoader, random_split\n# inspección de arquitectura\nfrom torchinfo import summary\n\n# directorio de datos\nDATA_DIR = '../data'\n\n# tamaño del lote\nBATCH_SIZE = 32\n# tamaño del vector de características\nFEAT_SIZE = 1024\n\n# reproducibilidad\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch_gen = torch.manual_seed(SEED)","metadata":{"colab":{},"colab_type":"code","id":"Ny0L2LzogTN-","execution":{"iopub.status.busy":"2022-11-14T02:58:52.086768Z","iopub.execute_input":"2022-11-14T02:58:52.087041Z","iopub.status.idle":"2022-11-14T02:58:54.059003Z","shell.execute_reply.started":"2022-11-14T02:58:52.087016Z","shell.execute_reply":"2022-11-14T02:58:54.058022Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2 Datos","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Conjunto de datos","metadata":{}},{"cell_type":"code","source":"class UCF11:\n\n    def __init__(self, root, download=False):\n        self.root = root\n        self.zarr_dir = join(root, 'ucf11.zarr')\n        if download:\n            self.download()\n        self.z = zarr.open(self.zarr_dir, 'r')\n        self.paths = list(self.z.array_keys())\n        \n    def __getitem__(self, i):\n        arr = self.z[self.paths[i]]\n        # [10, 1024] 10 cuadros por 1024 características cada uno\n        x = np.array(arr)\n        # [1] etiqueta de salida ej. número del 1 al 10 posibles etiquetas \n        y = np.array(arr.attrs['y'], dtype=np.int64)\n        return x, y\n\n    def __len__(self):\n        return len(self.paths)\n    \n    def _check_integrity(self):\n        return os.path.isdir(self.zarr_dir)\n    \n    def _extract(self, root, filename):\n        tar = tarfile.open(join(root, filename), \"r:gz\")\n        tar.extractall(root)\n        tar.close()\n\n    def download(self):\n        if self._check_integrity():\n            print('Files already downloaded and verified')\n            return\n        tvu.download_url(\n            url='https://cloud.xibalba.com.mx/s/apYrNA4iM4K65o7/download',\n            root=self.root,\n            filename='ucf11.zarr.tar.gz',\n            md5='c8a82454f9ec092d00bcd99c849e03fd'\n        )\n        self._extract(self.root, 'ucf11.zarr.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:58:54.060762Z","iopub.execute_input":"2022-11-14T02:58:54.062055Z","iopub.status.idle":"2022-11-14T02:58:54.071351Z","shell.execute_reply.started":"2022-11-14T02:58:54.062005Z","shell.execute_reply":"2022-11-14T02:58:54.070135Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Instancia del conjunto y partición","metadata":{}},{"cell_type":"code","source":"ds = UCF11(join(DATA_DIR, 'ucf11'), True)\nx, y = ds[0]\nprint(f'x shape={x.shape} dtype={x.dtype}')\nprint(f'x [0][:5]={x[0][:5]}')\nprint(f'y shape={y.shape} dtype={y.dtype} {y}')\nprint(f'y {y}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:58:54.073704Z","iopub.execute_input":"2022-11-14T02:58:54.074022Z","iopub.status.idle":"2022-11-14T02:59:22.065802Z","shell.execute_reply.started":"2022-11-14T02:58:54.073996Z","shell.execute_reply":"2022-11-14T02:59:22.064446Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading https://cloud.xibalba.com.mx/s/apYrNA4iM4K65o7/download to ../data/ucf11/ucf11.zarr.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/53436566 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e23e0c2e584f7bbe7020a47f8f138d"}},"metadata":{}},{"name":"stdout","text":"x shape=(10, 1024) dtype=float32\nx [0][:5]=[0.00022111 0.00368518 0.00314753 0.00201778 0.09296297]\ny shape=() dtype=int64 0\ny 0\n","output_type":"stream"}]},{"cell_type":"code","source":"trn_size = int(0.8 * len(ds))\ntst_size = len(ds) - trn_size\ntrn_ds, tst_ds = random_split(ds, [trn_size, tst_size])\nlen(trn_ds), len(tst_ds)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.067556Z","iopub.execute_input":"2022-11-14T02:59:22.067977Z","iopub.status.idle":"2022-11-14T02:59:22.079101Z","shell.execute_reply.started":"2022-11-14T02:59:22.067942Z","shell.execute_reply":"2022-11-14T02:59:22.078045Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(1279, 320)"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.3 Cargadores de datos","metadata":{}},{"cell_type":"code","source":"trn_dl = DataLoader(\n    # conjunto\n    trn_ds,\n    # tamaño del lote\n    batch_size=BATCH_SIZE,\n    # desordenar\n    shuffle=True,\n    # procesos paralelos\n    num_workers=2\n)\ntst_dl = DataLoader(\n    # conjunto\n    tst_ds,\n    # tamaño del lote\n    batch_size=BATCH_SIZE,\n    # desordenar\n    shuffle=True,\n    # procesos paralelos\n    num_workers=2\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.080437Z","iopub.execute_input":"2022-11-14T02:59:22.080780Z","iopub.status.idle":"2022-11-14T02:59:22.291142Z","shell.execute_reply.started":"2022-11-14T02:59:22.080750Z","shell.execute_reply":"2022-11-14T02:59:22.290246Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x, y = next(iter(trn_dl))\nprint(f'x shape={x.shape} dtype={x.dtype}')\nprint(f'y shape={y.shape} dtype={y.dtype}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.292070Z","iopub.execute_input":"2022-11-14T02:59:22.292650Z","iopub.status.idle":"2022-11-14T02:59:22.421809Z","shell.execute_reply.started":"2022-11-14T02:59:22.292625Z","shell.execute_reply":"2022-11-14T02:59:22.421045Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"x shape=torch.Size([32, 10, 1024]) dtype=torch.float32\ny shape=torch.Size([32]) dtype=torch.int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3 Modelo\n\n<!-- Torchvision provee una familia de [modelos](https://pytorch.org/docs/1.6.0/torchvision/models.html#classification) preentrenados en ImageNet. Usaremos [Shufflenet V2](https://arxiv.org/abs/1807.11164), una arquitectura eficiente para clasificación de imágenes.  -->","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Definición de arquitectura","metadata":{}},{"cell_type":"markdown","source":"### 3.1.1 Definición de arquitectura bidireccional RNN\n\n<center><img src=\"https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\"/></center>","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n\n    def __init__(self, input_size=1024, hidden_size=128, num_classes=11):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(input_size)\n        # input_size es el tamaño de la secuencia de entrada\n        # hidden_size es el tamaño de la memoria interna de la celda y que produce a la salida\n        # num_layers es el número de capaz que se apilan\n        # batch_first es que espera el tamaño del lote al inicio\n        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n                          num_layers=1, batch_first=True, bidirectional=True)\n        self.cls = nn.Linear(hidden_size*2, num_classes)\n\n    def forward(self, x):\n        # Batch, Seq, Feats, Hidden\n        # [32, 10, 1024]\n        \n        # [B, S, F] => [B, F, S]\n        x = x.movedim(1, 2)\n        # [B, F, S]\n        x = self.bn(x)\n        # [B, F, S] => [B, S, F]\n        x = x.movedim(1, 2)\n        # [B, S, F] => [B, S, H]\n        # [32, 10, 1024] => [32, 10, 128*2 (hidden_size*2)]\n        x, _ = self.rnn(x)\n        # [B, S, H] => [B, H]\n        # [32, 10, 256] => [32, 256] toma el último paso.\n        # Se pueden aplicar distintas ténicas de submuestreo\n        # 1. Tomar el último paso de la salida\n        #x = x[:, -1, :] #Tomar todo el lote, el último paso de la secuencia, y todas las características.\n        x = torch.mean(x,1) #2. Promedio\n        #x = torch.max(x,1) #3. Max pooling\n        # [B, H] = [B, 11]\n        x = self.cls(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.422745Z","iopub.execute_input":"2022-11-14T02:59:22.423051Z","iopub.status.idle":"2022-11-14T02:59:22.431470Z","shell.execute_reply.started":"2022-11-14T02:59:22.423024Z","shell.execute_reply":"2022-11-14T02:59:22.430134Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"modelRNN = RNN().eval()\nmodelRNN(torch.zeros(1, 10, 1024)).shape","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.433243Z","iopub.execute_input":"2022-11-14T02:59:22.434080Z","iopub.status.idle":"2022-11-14T02:59:22.497526Z","shell.execute_reply.started":"2022-11-14T02:59:22.434036Z","shell.execute_reply":"2022-11-14T02:59:22.496738Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 11])"},"metadata":{}}]},{"cell_type":"code","source":"summary(modelRNN, (1, 10, 1024), device='cpu', verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.501214Z","iopub.execute_input":"2022-11-14T02:59:22.501561Z","iopub.status.idle":"2022-11-14T02:59:22.514038Z","shell.execute_reply.started":"2022-11-14T02:59:22.501532Z","shell.execute_reply":"2022-11-14T02:59:22.513136Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nRNN                                      [1, 11]                   --\n├─BatchNorm1d: 1-1                       [1, 1024, 10]             2,048\n├─GRU: 1-2                               [1, 10, 256]              886,272\n├─Linear: 1-3                            [1, 11]                   2,827\n==========================================================================================\nTotal params: 891,147\nTrainable params: 891,147\nNon-trainable params: 0\nTotal mult-adds (M): 8.87\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.10\nParams size (MB): 3.56\nEstimated Total Size (MB): 3.71\n=========================================================================================="},"metadata":{}}]},{"cell_type":"markdown","source":"## 3.1.2 Definición de la arquitectura CNN","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n\n    def __init__(self, input_size=1024, in_channels=10, num_classes=11):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(input_size)\n        self.cnn = nn.Sequential(\n            # bloque conv1\n            # [B, S, F]\n            # [32, 10, 1024] => [32, 10, 1024]\n            nn.Conv1d(in_channels=10, out_channels=10, kernel_size=3, padding=1),\n            # [32, 10, 1024] = [32, 10, 1024]\n            nn.ReLU(),\n            # [32, 10, 1024] => [32, 10, 256]\n            nn.MaxPool2d(kernel_size=2, stride=4),\n        )\n        self.cls = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        # Batch, Seq, Feats, Hidden\n        # [32, 10, 1024]\n        \n        # [B, S, F] => [B, F, S]\n        x = x.movedim(1, 2)\n        # [B, F, S]\n        x = self.bn(x)\n        # [B, F, S] => [B, S, F]\n        x = x.movedim(1, 2)\n        # [B, S, F] => [B, S, H]\n        # [32, 10, 1024] => [32, 10, 256]\n        x = self.cnn(x)\n        # [B, S, H] => [B, H]\n        # [32, 10, 256] => [32, 256] toma el último paso.\n        # Se pueden aplicar distintas ténicas de submuestreo\n        # 1. Tomar el último paso de la salida\n        # 2. \n        #x = x[:, -1, :] #Tomar todo el lote, el último paso de la secuencia, y todas las características.\n        x = torch.mean(x,1) #Promedio\n        #x = torch.max(x,1) #Max pooling\n        # [B, H] = [B, 11]\n        x = self.cls(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.515188Z","iopub.execute_input":"2022-11-14T02:59:22.515463Z","iopub.status.idle":"2022-11-14T02:59:22.523255Z","shell.execute_reply.started":"2022-11-14T02:59:22.515437Z","shell.execute_reply":"2022-11-14T02:59:22.522395Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"modelCNN = CNN().eval()\nmodelCNN(torch.zeros(1, 10, 1024)).shape","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.524318Z","iopub.execute_input":"2022-11-14T02:59:22.524630Z","iopub.status.idle":"2022-11-14T02:59:22.550455Z","shell.execute_reply.started":"2022-11-14T02:59:22.524600Z","shell.execute_reply":"2022-11-14T02:59:22.549439Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 11])"},"metadata":{}}]},{"cell_type":"code","source":"summary(modelCNN, (1, 10, 1024), device='cpu', verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.551757Z","iopub.execute_input":"2022-11-14T02:59:22.552064Z","iopub.status.idle":"2022-11-14T02:59:22.561298Z","shell.execute_reply.started":"2022-11-14T02:59:22.552035Z","shell.execute_reply":"2022-11-14T02:59:22.560259Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 11]                   --\n├─BatchNorm1d: 1-1                       [1, 1024, 10]             2,048\n├─Sequential: 1-2                        [1, 3, 256]               --\n│    └─Conv1d: 2-1                       [1, 10, 1024]             310\n│    └─ReLU: 2-2                         [1, 10, 1024]             --\n│    └─MaxPool2d: 2-3                    [1, 3, 256]               --\n├─Linear: 1-3                            [1, 11]                   2,827\n==========================================================================================\nTotal params: 5,185\nTrainable params: 5,185\nNon-trainable params: 0\nTotal mult-adds (M): 0.32\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.16\nParams size (MB): 0.02\nEstimated Total Size (MB): 0.23\n=========================================================================================="},"metadata":{}}]},{"cell_type":"markdown","source":"## 4 Entrenamiento","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Ciclo de entrenamiento","metadata":{}},{"cell_type":"markdown","source":"### 4.1.1 Entrenamiento con RNN","metadata":{}},{"cell_type":"code","source":"# optimizador\nopt = optim.Adam(modelRNN.parameters(), lr=1e-3)\n\n# ciclo de entrenamiento\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n\n    # modelo en modo de entrenamiento\n    modelRNN.train()\n    \n    # entrenamiento de una época\n    for x, y_true in trn_dl:\n        # hacemos inferencia para obtener los logits\n        y_lgts = modelRNN(x)\n        # calculamos la pérdida\n        loss = F.cross_entropy(y_lgts, y_true)\n        # vaciamos los gradientes\n        opt.zero_grad()\n        # retropropagamos\n        loss.backward()\n        # actulizamos parámetros\n        opt.step()\n\n    # desactivamos temporalmente la gráfica de cómputo\n    with torch.no_grad():\n\n        # modelo en modo de evaluación\n        modelRNN.eval()\n        \n        losses, accs = [], []\n        # validación de la época\n        for x, y_true in tst_dl:\n            # hacemos inferencia para obtener los logits\n            y_lgts = modelRNN(x)\n            # calculamos las probabilidades\n            y_prob = F.softmax(y_lgts, 1)\n            # obtenemos la clase predicha\n            y_pred = torch.argmax(y_prob, 1)\n            \n            # calculamos la pérdida\n            loss = F.cross_entropy(y_lgts, y_true)\n            # calculamos la exactitud\n            acc = (y_true == y_pred).type(torch.float32).mean()\n\n            # guardamos históricos\n            losses.append(loss.item() * 100)\n            accs.append(acc.item() * 100)\n\n        # imprimimos métricas\n        loss = np.mean(losses)\n        acc = np.mean(accs)\n        print(f'E{epoch:2} loss={loss:6.2f} acc={acc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:22.562579Z","iopub.execute_input":"2022-11-14T02:59:22.563989Z","iopub.status.idle":"2022-11-14T02:59:50.514371Z","shell.execute_reply.started":"2022-11-14T02:59:22.563952Z","shell.execute_reply":"2022-11-14T02:59:50.512527Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"E 0 loss=190.82 acc=37.19\nE 1 loss=161.14 acc=48.44\nE 2 loss=147.69 acc=52.81\nE 3 loss=140.66 acc=54.38\nE 4 loss=132.86 acc=57.81\nE 5 loss=126.65 acc=60.31\nE 6 loss=129.13 acc=61.25\nE 7 loss=132.95 acc=60.31\nE 8 loss=127.48 acc=61.25\nE 9 loss=134.98 acc=60.31\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4.1.2 Entrenamiento con CNN","metadata":{}},{"cell_type":"code","source":"# optimizador\nopt = optim.Adam(modelCNN.parameters(), lr=1e-3)\n\n# ciclo de entrenamiento\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n\n    # modelo en modo de entrenamiento\n    modelCNN.train()\n    \n    # entrenamiento de una época\n    for x, y_true in trn_dl:\n        # hacemos inferencia para obtener los logits\n        y_lgts = modelCNN(x)\n        # calculamos la pérdida\n        loss = F.cross_entropy(y_lgts, y_true)\n        # vaciamos los gradientes\n        opt.zero_grad()\n        # retropropagamos\n        loss.backward()\n        # actulizamos parámetros\n        opt.step()\n\n    # desactivamos temporalmente la gráfica de cómputo\n    with torch.no_grad():\n\n        # modelo en modo de evaluación\n        modelCNN.eval()\n        \n        losses, accs = [], []\n        # validación de la época\n        for x, y_true in tst_dl:\n            # hacemos inferencia para obtener los logits\n            y_lgts = modelCNN(x)\n            # calculamos las probabilidades\n            y_prob = F.softmax(y_lgts, 1)\n            # obtenemos la clase predicha\n            y_pred = torch.argmax(y_prob, 1)\n            \n            # calculamos la pérdida\n            loss = F.cross_entropy(y_lgts, y_true)\n            # calculamos la exactitud\n            acc = (y_true == y_pred).type(torch.float32).mean()\n\n            # guardamos históricos\n            losses.append(loss.item() * 100)\n            accs.append(acc.item() * 100)\n\n        # imprimimos métricas\n        loss = np.mean(losses)\n        acc = np.mean(accs)\n        print(f'E{epoch:2} loss={loss:6.2f} acc={acc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T02:59:50.515815Z","iopub.execute_input":"2022-11-14T02:59:50.516131Z","iopub.status.idle":"2022-11-14T03:00:00.277565Z","shell.execute_reply.started":"2022-11-14T02:59:50.516101Z","shell.execute_reply":"2022-11-14T03:00:00.276427Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"E 0 loss=234.68 acc=16.56\nE 1 loss=224.12 acc=24.06\nE 2 loss=210.69 acc=30.00\nE 3 loss=198.80 acc=35.31\nE 4 loss=192.71 acc=36.25\nE 5 loss=185.33 acc=39.38\nE 6 loss=177.46 acc=38.75\nE 7 loss=170.46 acc=44.69\nE 8 loss=168.46 acc=43.44\nE 9 loss=162.34 acc=43.75\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Modificación para que CNN tenga un número silimar de parámetros que la RNN (891,147)","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Definición de la nueva arquitectura CNN","metadata":{}},{"cell_type":"code","source":"class CNN2(nn.Module):\n\n    def __init__(self, input_size=1024, in_channels=10, num_classes=11):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(input_size)\n        self.cnn = nn.Sequential(\n            # bloque conv1\n            # [B, S, F]\n            # [32, 10, 1024] => [32, 10, 1024]\n            nn.Conv1d(in_channels=10, out_channels=10, kernel_size=3, padding=1),\n            # [32, 10, 1024] = [32, 10, 1024]\n            nn.ReLU(),\n            # [32, 10, 1024] => [32, 10, 1024]\n            nn.MaxPool2d(kernel_size=3, stride=1),\n        )\n        self.cls0 = nn.Linear(1022, 512)\n        self.relu1 = nn.ReLU()\n        self.cls1 = nn.Linear(512, 256)\n        self.relu2 = nn.ReLU()\n        self.cls = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        # Batch, Seq, Feats, Hidden\n        # [32, 10, 1024]\n        \n        # [B, S, F] => [B, F, S]\n        x = x.movedim(1, 2)\n        # [B, F, S]\n        x = self.bn(x)\n        # [B, F, S] => [B, S, F]\n        x = x.movedim(1, 2)\n        # [B, S, F] => [B, S, H]\n        # [32, 10, 1024] => [32, 10, 1024]\n        x = self.cnn(x)\n        # [B, S, H] => [B, H]\n        # [32, 10, 256] => [32, 256] toma el último paso.\n        # Se pueden aplicar distintas ténicas de submuestreo\n        # 1. Tomar el último paso de la salida\n        # 2. \n        #x = x[:, -1, :] #Tomar todo el lote, el último paso de la secuencia, y todas las características.\n        x = torch.mean(x,1) #Promedio\n        #x = torch.max(x,1) #Max pooling\n        # [B, H] = [B, 11]\n        x = self.cls0(x)\n        x = self.relu1(x)\n        x = self.cls1(x)\n        x = self.relu2(x)\n        x = self.cls(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:00:00.279124Z","iopub.execute_input":"2022-11-14T03:00:00.279503Z","iopub.status.idle":"2022-11-14T03:00:00.290258Z","shell.execute_reply.started":"2022-11-14T03:00:00.279458Z","shell.execute_reply":"2022-11-14T03:00:00.289218Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"modelCNN2 = CNN2().eval()\nmodelCNN2(torch.zeros(1, 10, 1024)).shape","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:00:00.292592Z","iopub.execute_input":"2022-11-14T03:00:00.293284Z","iopub.status.idle":"2022-11-14T03:00:00.321861Z","shell.execute_reply.started":"2022-11-14T03:00:00.293243Z","shell.execute_reply":"2022-11-14T03:00:00.320535Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 11])"},"metadata":{}}]},{"cell_type":"code","source":"summary(modelCNN2, (1, 10, 1024), device='cpu', verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:00:00.323256Z","iopub.execute_input":"2022-11-14T03:00:00.323682Z","iopub.status.idle":"2022-11-14T03:00:00.337095Z","shell.execute_reply.started":"2022-11-14T03:00:00.323653Z","shell.execute_reply":"2022-11-14T03:00:00.336108Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN2                                     [1, 11]                   --\n├─BatchNorm1d: 1-1                       [1, 1024, 10]             2,048\n├─Sequential: 1-2                        [1, 8, 1022]              --\n│    └─Conv1d: 2-1                       [1, 10, 1024]             310\n│    └─ReLU: 2-2                         [1, 10, 1024]             --\n│    └─MaxPool2d: 2-3                    [1, 8, 1022]              --\n├─Linear: 1-3                            [1, 512]                  523,776\n├─ReLU: 1-4                              [1, 512]                  --\n├─Linear: 1-5                            [1, 256]                  131,328\n├─ReLU: 1-6                              [1, 256]                  --\n├─Linear: 1-7                            [1, 11]                   2,827\n==========================================================================================\nTotal params: 660,289\nTrainable params: 660,289\nNon-trainable params: 0\nTotal mult-adds (M): 0.98\n==========================================================================================\nInput size (MB): 0.04\nForward/backward pass size (MB): 0.17\nParams size (MB): 2.64\nEstimated Total Size (MB): 2.85\n=========================================================================================="},"metadata":{}}]},{"cell_type":"markdown","source":"### 5.2 Entrenamiento con CNN2 (660,289)","metadata":{}},{"cell_type":"code","source":"# optimizador\nopt = optim.Adam(modelCNN2.parameters(), lr=1e-3)\n\n# ciclo de entrenamiento\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n\n    # modelo en modo de entrenamiento\n    modelCNN2.train()\n    \n    # entrenamiento de una época\n    for x, y_true in trn_dl:\n        # hacemos inferencia para obtener los logits\n        y_lgts = modelCNN2(x)\n        # calculamos la pérdida\n        loss = F.cross_entropy(y_lgts, y_true)\n        # vaciamos los gradientes\n        opt.zero_grad()\n        # retropropagamos\n        loss.backward()\n        # actulizamos parámetros\n        opt.step()\n\n    # desactivamos temporalmente la gráfica de cómputo\n    with torch.no_grad():\n\n        # modelo en modo de evaluación\n        modelCNN2.eval()\n        \n        losses, accs = [], []\n        # validación de la época\n        for x, y_true in tst_dl:\n            # hacemos inferencia para obtener los logits\n            y_lgts = modelCNN2(x)\n            # calculamos las probabilidades\n            y_prob = F.softmax(y_lgts, 1)\n            # obtenemos la clase predicha\n            y_pred = torch.argmax(y_prob, 1)\n            \n            # calculamos la pérdida\n            loss = F.cross_entropy(y_lgts, y_true)\n            # calculamos la exactitud\n            acc = (y_true == y_pred).type(torch.float32).mean()\n\n            # guardamos históricos\n            losses.append(loss.item() * 100)\n            accs.append(acc.item() * 100)\n\n        # imprimimos métricas\n        loss = np.mean(losses)\n        acc = np.mean(accs)\n        print(f'E{epoch:2} loss={loss:6.2f} acc={acc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:00:00.339765Z","iopub.execute_input":"2022-11-14T03:00:00.340065Z","iopub.status.idle":"2022-11-14T03:00:13.673658Z","shell.execute_reply.started":"2022-11-14T03:00:00.340033Z","shell.execute_reply":"2022-11-14T03:00:13.672897Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"E 0 loss=231.82 acc=22.50\nE 1 loss=221.68 acc=21.88\nE 2 loss=193.49 acc=36.56\nE 3 loss=181.37 acc=38.44\nE 4 loss=173.38 acc=43.75\nE 5 loss=158.50 acc=43.75\nE 6 loss=153.21 acc=51.56\nE 7 loss=148.79 acc=50.62\nE 8 loss=155.33 acc=52.19\nE 9 loss=142.54 acc=54.38\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusiones y discusión\n\nLa red neuronal recurrente (RNN) bidireccional con una capa GRU tiene en total 891,147 parámetros entrenables. La red neuronal convolucional (CNN) tiene un total de 5,185 parámetros entrenables.\n  \nLa RNN para el conjunto de prueba llega a una exactitud de 60.31. Y la CNN obtiene una exactitud de 43.75. Ambas en la época número 10.\n  \nUna de las razones por la que la RNN obtuvo una mejor métrica se debe a que se aprovecha la temporalidad de la información recibida, ya que es una secuencia de las características extraídas de 10 cuadros del video. Además, a pesar de que la secuencia es larga (1024) y que puede ocurrir desvanecimiento del gradiente, la RNN sigue obtiendo un buen rendimiento comparada con la CNN. También, se podría pensar que por el número de parámetros de la RNN puede llegar a existir sobre-ajuste, sin embargo, desde la época 5 parece que ya no se modifica la exactitud.\n  \nPor otro lado, para lograr incrementar el número de parámetros de la red convolucional, se agregaron capas ocultas a la capa densa con el objetivo. Llegando a un total de 660,289 parámetros entrenables. Al entrenar este segundo modelo se obtiene una exactitud de 54.38 en el conjunto de prueba, y compararlo con la RNN, se concluye que el número de parámetros mejoró la exactitud de la CNN, sin embargo, la RNN sigue obteniendo un mejor rendimiento.","metadata":{}}]}